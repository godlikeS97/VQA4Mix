{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified VQA4Mix Pipeline\n",
    "\n",
    "This notebook combines the functionality from all category-specific pipelines (food, painting, people, cat) into a single unified pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import skimage.io as io\n",
    "from PIL import Image\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data_processing.data_loader import load_json_data, load_annotation_data, save_json_data, convert_df_to_json\n",
    "from src.model.question_generator import generate_random_choice, generate_multiple_choice_question\n",
    "from src.utils.image_augmentation import load_image, apply_augmentations\n",
    "from src.utils.evaluation import calculate_accuracy, calculate_accuracy_by_difficulty, plot_accuracy_comparison\n",
    "from src.visualization.plotting import plot_confusion_matrix, plot_accuracy_by_category\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths for each category\n",
    "DATA_PATHS = {\n",
    "    'food': '../data/food/food_annotation.json',\n",
    "    'painting': '../data/painting/paintings.json',\n",
    "    'people': '../data/people/people_data.json',\n",
    "    'cat': '../data/cat/upking_data.json'\n",
    "}\n",
    "\n",
    "# Define output paths for processed data\n",
    "OUTPUT_PATHS = {\n",
    "    'food': '../data/food/food_annotation_with_MCQ_3_difficulies.json',\n",
    "    'painting': '../data/painting/paintings_with_MCQ_3diff.json',\n",
    "    'people': '../data/people/people_annotation_with_MCQ_3_difficulies.json',\n",
    "    'cat': '../data/cat/upking_annotation_with_MCQ_3_difficulies.json'\n",
    "}\n",
    "\n",
    "# Define output paths for results\n",
    "RESULT_PATHS = {\n",
    "    'food': '../data/food/food_annotation_with_MCQ_result_3_difficulties.json',\n",
    "    'painting': '../data/painting/paintings_with_MCQ_3diff_result.json',\n",
    "    'people': '../data/people/people_annotation_with_MCQ_result_3_difficulties.json',\n",
    "    'cat': '../data/cat/upking_annotation_with_MCQ_result_3_difficulties.json'\n",
    "}\n",
    "\n",
    "# Define output paths for augmented results\n",
    "AUGMENTED_RESULT_PATHS = {\n",
    "    'food': '../data/food/food_annotation_with_MCQ_result_3_difficulties_with_image_augmentation.json',\n",
    "    'painting': '../data/painting/paintings_with_MCQ_3diff_result_augmentation.json',\n",
    "    'people': '../data/people/people_annotation_with_MCQ_result_3_difficulties_with_image_augmentation.json',\n",
    "    'cat': '../data/cat/upking_annotation_with_MCQ_result_3_difficulties_with_image_augmentation.json'\n",
    "}\n",
    "\n",
    "# Define the category to process (set to None to process all categories)\n",
    "CATEGORY = None  # Options: 'food', 'painting', 'people', 'cat', or None for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_category_data(category):\n",
    "    \"\"\"Load data for a specific category.\"\"\"\n",
    "    file_path = DATA_PATHS[category]\n",
    "    print(f\"Loading {category} data from {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = load_annotation_data(file_path)\n",
    "        print(f\"Loaded {len(df)} records for {category}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {category} data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load data for the specified category or all categories\n",
    "category_data = {}\n",
    "if CATEGORY is not None:\n",
    "    category_data[CATEGORY] = load_category_data(CATEGORY)\n",
    "else:\n",
    "    for category in DATA_PATHS.keys():\n",
    "        category_data[category] = load_category_data(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first row of each category's data\n",
    "for category, df in category_data.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\n{category.upper()} DATA STRUCTURE:\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(f\"Sample row:\")\n",
    "        display(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Multiple Choice Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mcq_for_category(df, category, sample_size=None):\n",
    "    \"\"\"Generate multiple choice questions for a category.\"\"\"\n",
    "    print(f\"Generating MCQs for {category}...\")\n",
    "    \n",
    "    # Use a sample if specified\n",
    "    if sample_size is not None:\n",
    "        df = df.head(sample_size)\n",
    "    \n",
    "    # Generate random choices for each row\n",
    "    df['multiple_choice_solution'] = df.apply(lambda x: generate_random_choice(), axis=1)\n",
    "    \n",
    "    # Determine the caption column name based on the category\n",
    "    caption_col = 'captions' if 'captions' in df.columns else 'reference_caption'\n",
    "    \n",
    "    # Generate questions for each difficulty level\n",
    "    for level in ['easy', 'medium', 'hard']:\n",
    "        col_name = f'multiple_choice_question_{level}'\n",
    "        print(f\"Generating {level} questions...\")\n",
    "        \n",
    "        # Get the first caption if it's a list, otherwise use the caption directly\n",
    "        df[col_name] = df.apply(\n",
    "            lambda x: generate_multiple_choice_question(\n",
    "                x[caption_col][0] if isinstance(x[caption_col], list) else x[caption_col],\n",
    "                x['multiple_choice_solution'],\n",
    "                level=level\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Save the results\n",
    "    output_path = OUTPUT_PATHS[category]\n",
    "    convert_df_to_json(df, output_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate MCQs for each category\n",
    "mcq_data = {}\n",
    "for category, df in category_data.items():\n",
    "    if df is not None:\n",
    "        # Use a small sample size for demonstration\n",
    "        sample_size = 5  # Set to None to process all data\n",
    "        mcq_data[category] = generate_mcq_for_category(df, category, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLaVA Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.inference import load_llava_model, perform_multiple_choice_task\n",
    "\n",
    "# Load the LLaVA model\n",
    "model_path = '/path/to/llava-model'  # Update with the actual model path\n",
    "try:\n",
    "    processor, model = load_llava_model(model_path)\n",
    "    print(\"LLaVA model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLaVA model: {e}\")\n",
    "    processor, model = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Multiple Choice Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mcq_task_for_category(df, category, processor, model, use_augmentation=False):\n",
    "    \"\"\"Perform multiple choice task for a category.\"\"\"\n",
    "    if processor is None or model is None:\n",
    "        print(\"Model not loaded. Skipping inference.\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Performing MCQ task for {category}...\")\n",
    "    \n",
    "    # Determine the image path column name based on the category\n",
    "    img_path_col = 'file_path' if 'file_path' in df.columns else 'img_url'\n",
    "    \n",
    "    # Perform inference for each difficulty level\n",
    "    for level in ['easy', 'medium', 'hard']:\n",
    "        question_col = f'multiple_choice_question_{level}'\n",
    "        prediction_col = f'multiple_choice_prediction_{level}'\n",
    "        \n",
    "        print(f\"Processing {level} questions...\")\n",
    "        \n",
    "        # Perform inference for each row\n",
    "        predictions = []\n",
    "        for _, row in df.iterrows():\n",
    "            img_path = row[img_path_col]\n",
    "            question = row[question_col]\n",
    "            \n",
    "            # Apply augmentation if specified\n",
    "            if use_augmentation:\n",
    "                img = load_image(img_path)\n",
    "                augmented_img = apply_augmentations(img)\n",
    "                # Save augmented image to a temporary file\n",
    "                temp_path = f\"temp_augmented_{random.randint(1000, 9999)}.jpg\"\n",
    "                Image.fromarray(augmented_img).save(temp_path)\n",
    "                img_path = temp_path\n",
    "            \n",
    "            # Perform inference\n",
    "            try:\n",
    "                answer = perform_multiple_choice_task(processor, model, img_path, question)\n",
    "                predictions.append(answer)\n",
    "            except Exception as e:\n",
    "                print(f\"Error performing inference: {e}\")\n",
    "                predictions.append(None)\n",
    "            \n",
    "            # Remove temporary file if created\n",
    "            if use_augmentation and os.path.exists(temp_path):\n",
    "                os.remove(temp_path)\n",
    "        \n",
    "        # Add predictions to the DataFrame\n",
    "        df[prediction_col] = predictions\n",
    "    \n",
    "    # Save the results\n",
    "    output_path = AUGMENTED_RESULT_PATHS[category] if use_augmentation else RESULT_PATHS[category]\n",
    "    convert_df_to_json(df, output_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Perform MCQ task for each category\n",
    "result_data = {}\n",
    "for category, df in mcq_data.items():\n",
    "    if df is not None:\n",
    "        # Set use_augmentation to True to use image augmentation\n",
    "        use_augmentation = False\n",
    "        result_data[category] = perform_mcq_task_for_category(df, category, processor, model, use_augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results_for_category(df, category):\n",
    "    \"\"\"Evaluate results for a category.\"\"\"\n",
    "    print(f\"Evaluating results for {category}...\")\n",
    "    \n",
    "    # Define prediction columns for each difficulty level\n",
    "    prediction_cols = {\n",
    "        'easy': 'multiple_choice_prediction_easy',\n",
    "        'medium': 'multiple_choice_prediction_medium',\n",
    "        'hard': 'multiple_choice_prediction_hard'\n",
    "    }\n",
    "    \n",
    "    # Calculate accuracy for each difficulty level\n",
    "    accuracies = {}\n",
    "    for level, col in prediction_cols.items():\n",
    "        if col in df.columns:\n",
    "            accuracy = calculate_accuracy(df[col], df['multiple_choice_solution'])\n",
    "            accuracies[level] = accuracy\n",
    "            print(f\"{level.capitalize()} accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Plot accuracy comparison\n",
    "    if accuracies:\n",
    "        fig = plot_accuracy_comparison(accuracies, title=f\"{category.capitalize()} - Accuracy by Difficulty\")\n",
    "        plt.show()\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Evaluate results for each category\n",
    "evaluation_results = {}\n",
    "for category, df in result_data.items():\n",
    "    if df is not None:\n",
    "        evaluation_results[category] = evaluate_results_for_category(df, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results Across Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results_across_categories(evaluation_results):\n",
    "    \"\"\"Compare results across categories.\"\"\"\n",
    "    if not evaluation_results:\n",
    "        print(\"No evaluation results to compare.\")\n",
    "        return\n",
    "    \n",
    "    # Compare results for each difficulty level\n",
    "    for level in ['easy', 'medium', 'hard']:\n",
    "        # Collect accuracies for each category\n",
    "        category_accuracies = {}\n",
    "        for category, accuracies in evaluation_results.items():\n",
    "            if level in accuracies:\n",
    "                category_accuracies[category] = accuracies[level]\n",
    "        \n",
    "        # Plot comparison\n",
    "        if category_accuracies:\n",
    "            fig = plot_accuracy_comparison(category_accuracies, title=f\"{level.capitalize()} Difficulty - Accuracy by Category\")\n",
    "            plt.show()\n",
    "\n",
    "# Compare results across categories\n",
    "compare_results_across_categories(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This unified pipeline combines the functionality from all category-specific pipelines into a single workflow. It allows for processing data from multiple categories, generating multiple-choice questions, performing inference with the LLaVA model, and evaluating the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
