{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis for VQA4Mix\n",
    "\n",
    "This notebook analyzes the results of the VQA4Mix project, including:\n",
    "- Loading and examining results from different categories\n",
    "- Calculating accuracy metrics\n",
    "- Visualizing results\n",
    "- Comparing performance across categories and difficulty levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data_processing.data_loader import load_json_data, load_annotation_data\n",
    "from src.utils.evaluation import calculate_accuracy, calculate_accuracy_by_difficulty, generate_confusion_matrix\n",
    "from src.visualization.plotting import plot_confusion_matrix, plot_accuracy_by_category, plot_model_comparison\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define result paths for each category\n",
    "RESULT_PATHS = {\n",
    "    'food': '../data/food/food_annotation_with_MCQ_result_3_difficulties.json',\n",
    "    'painting': '../data/painting/paintings_with_MCQ_3diff_result.json',\n",
    "    'people': '../data/people/people_annotation_with_MCQ_result_3_difficulties.json',\n",
    "    'cat': '../data/cat/upking_annotation_with_MCQ_result_3_difficulties.json'\n",
    "}\n",
    "\n",
    "# Define augmented result paths for each category\n",
    "AUGMENTED_RESULT_PATHS = {\n",
    "    'food': '../data/food/food_annotation_with_MCQ_result_3_difficulties_with_image_augmentation.json',\n",
    "    'painting': '../data/painting/paintings_with_MCQ_3diff_result_augmentation.json',\n",
    "    'people': '../data/people/people_annotation_with_MCQ_result_3_difficulties_with_image_augmentation.json',\n",
    "    'cat': '../data/cat/upking_annotation_with_MCQ_result_3_difficulties_with_image_augmentation.json'\n",
    "}\n",
    "\n",
    "# Define the category to analyze (set to None to analyze all categories)\n",
    "CATEGORY = None  # Options: 'food', 'painting', 'people', 'cat', or None for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result_data(category, augmented=False):\n",
    "    \"\"\"Load result data for a specific category.\"\"\"\n",
    "    file_path = AUGMENTED_RESULT_PATHS[category] if augmented else RESULT_PATHS[category]\n",
    "    print(f\"Loading {'augmented ' if augmented else ''}results for {category} from {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = load_annotation_data(file_path)\n",
    "        print(f\"Loaded {len(df)} records for {category}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results for {category}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load results for the specified category or all categories\n",
    "result_data = {}\n",
    "augmented_result_data = {}\n",
    "\n",
    "if CATEGORY is not None:\n",
    "    result_data[CATEGORY] = load_result_data(CATEGORY, augmented=False)\n",
    "    augmented_result_data[CATEGORY] = load_result_data(CATEGORY, augmented=True)\n",
    "else:\n",
    "    for category in RESULT_PATHS.keys():\n",
    "        result_data[category] = load_result_data(category, augmented=False)\n",
    "        augmented_result_data[category] = load_result_data(category, augmented=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Result Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first row of each category's results\n",
    "for category, df in result_data.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\n{category.upper()} RESULT STRUCTURE:\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(f\"Sample row:\")\n",
    "        display(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, category, augmented=False):\n",
    "    \"\"\"Calculate accuracy metrics for a category.\"\"\"\n",
    "    print(f\"Calculating metrics for {category} ({'augmented' if augmented else 'standard'})...\")\n",
    "    \n",
    "    # Define prediction columns for each difficulty level\n",
    "    prediction_cols = {\n",
    "        'easy': 'multiple_choice_prediction_easy',\n",
    "        'medium': 'multiple_choice_prediction_medium',\n",
    "        'hard': 'multiple_choice_prediction_hard'\n",
    "    }\n",
    "    \n",
    "    # Calculate accuracy for each difficulty level\n",
    "    accuracies = {}\n",
    "    for level, col in prediction_cols.items():\n",
    "        if col in df.columns:\n",
    "            accuracy = calculate_accuracy(df[col], df['multiple_choice_solution'])\n",
    "            accuracies[level] = accuracy\n",
    "            print(f\"{level.capitalize()} accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Calculate metrics for each category\n",
    "metrics = {}\n",
    "augmented_metrics = {}\n",
    "\n",
    "for category, df in result_data.items():\n",
    "    if df is not None:\n",
    "        metrics[category] = calculate_metrics(df, category, augmented=False)\n",
    "\n",
    "for category, df in augmented_result_data.items():\n",
    "    if df is not None:\n",
    "        augmented_metrics[category] = calculate_metrics(df, category, augmented=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results_by_category(metrics, title_prefix=\"\"):\n",
    "    \"\"\"Visualize results by category.\"\"\"\n",
    "    if not metrics:\n",
    "        print(\"No metrics to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Visualize results for each difficulty level\n",
    "    for level in ['easy', 'medium', 'hard']:\n",
    "        # Collect accuracies for each category\n",
    "        category_accuracies = {}\n",
    "        for category, accuracies in metrics.items():\n",
    "            if level in accuracies:\n",
    "                category_accuracies[category] = accuracies[level]\n",
    "        \n",
    "        # Plot comparison\n",
    "        if category_accuracies:\n",
    "            title = f\"{title_prefix}{level.capitalize()} Difficulty - Accuracy by Category\"\n",
    "            fig = plot_accuracy_by_category(\n",
    "                list(category_accuracies.values()),\n",
    "                list(category_accuracies.keys()),\n",
    "                title=title\n",
    "            )\n",
    "            plt.show()\n",
    "\n",
    "# Visualize results by category\n",
    "visualize_results_by_category(metrics, title_prefix=\"Standard - \")\n",
    "visualize_results_by_category(augmented_metrics, title_prefix=\"Augmented - \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results by Difficulty Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results_by_difficulty(metrics, augmented_metrics):\n",
    "    \"\"\"Visualize results by difficulty level.\"\"\"\n",
    "    if not metrics or not augmented_metrics:\n",
    "        print(\"Insufficient metrics to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Visualize results for each category\n",
    "    for category in metrics.keys():\n",
    "        if category in augmented_metrics:\n",
    "            std_accuracies = metrics[category]\n",
    "            aug_accuracies = augmented_metrics[category]\n",
    "            \n",
    "            # Combine standard and augmented results\n",
    "            combined_accuracies = {}\n",
    "            for level in ['easy', 'medium', 'hard']:\n",
    "                if level in std_accuracies:\n",
    "                    combined_accuracies[f\"Standard - {level.capitalize()}\"] = std_accuracies[level]\n",
    "                if level in aug_accuracies:\n",
    "                    combined_accuracies[f\"Augmented - {level.capitalize()}\"] = aug_accuracies[level]\n",
    "            \n",
    "            # Plot comparison\n",
    "            if combined_accuracies:\n",
    "                title = f\"{category.capitalize()} - Accuracy by Difficulty Level and Augmentation\"\n",
    "                fig = plot_model_comparison(combined_accuracies, title=title)\n",
    "                plt.show()\n",
    "\n",
    "# Visualize results by difficulty level\n",
    "visualize_results_by_difficulty(metrics, augmented_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_plot_confusion_matrices(df, category, augmented=False):\n",
    "    \"\"\"Generate and plot confusion matrices for a category.\"\"\"\n",
    "    print(f\"Generating confusion matrices for {category} ({'augmented' if augmented else 'standard'})...\")\n",
    "    \n",
    "    # Define prediction columns for each difficulty level\n",
    "    prediction_cols = {\n",
    "        'easy': 'multiple_choice_prediction_easy',\n",
    "        'medium': 'multiple_choice_prediction_medium',\n",
    "        'hard': 'multiple_choice_prediction_hard'\n",
    "    }\n",
    "    \n",
    "    # Generate and plot confusion matrix for each difficulty level\n",
    "    for level, col in prediction_cols.items():\n",
    "        if col in df.columns:\n",
    "            # Generate confusion matrix\n",
    "            confusion_mat = generate_confusion_matrix(df[col], df['multiple_choice_solution'])\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            title = f\"{category.capitalize()} - {level.capitalize()} Difficulty ({'Augmented' if augmented else 'Standard'})\"\n",
    "            fig = plot_confusion_matrix(confusion_mat, title=title)\n",
    "            plt.show()\n",
    "\n",
    "# Generate and plot confusion matrices for each category\n",
    "for category, df in result_data.items():\n",
    "    if df is not None:\n",
    "        generate_and_plot_confusion_matrices(df, category, augmented=False)\n",
    "\n",
    "for category, df in augmented_result_data.items():\n",
    "    if df is not None:\n",
    "        generate_and_plot_confusion_matrices(df, category, augmented=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Standard vs. Augmented Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_standard_vs_augmented(metrics, augmented_metrics):\n",
    "    \"\"\"Compare standard vs. augmented results.\"\"\"\n",
    "    if not metrics or not augmented_metrics:\n",
    "        print(\"Insufficient metrics to compare.\")\n",
    "        return\n",
    "    \n",
    "    # Compare results for each difficulty level\n",
    "    for level in ['easy', 'medium', 'hard']:\n",
    "        # Collect accuracies for each category\n",
    "        comparison = {}\n",
    "        for category in metrics.keys():\n",
    "            if category in augmented_metrics and level in metrics[category] and level in augmented_metrics[category]:\n",
    "                std_acc = metrics[category][level]\n",
    "                aug_acc = augmented_metrics[category][level]\n",
    "                comparison[category] = {\n",
    "                    'Standard': std_acc,\n",
    "                    'Augmented': aug_acc,\n",
    "                    'Difference': aug_acc - std_acc\n",
    "                }\n",
    "        \n",
    "        # Create a DataFrame for comparison\n",
    "        if comparison:\n",
    "            df_comparison = pd.DataFrame(comparison).T\n",
    "            df_comparison['Difference %'] = df_comparison['Difference'] * 100\n",
    "            \n",
    "            print(f\"\\nComparison for {level.capitalize()} Difficulty:\")\n",
    "            display(df_comparison)\n",
    "            \n",
    "            # Plot comparison\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            df_comparison[['Standard', 'Augmented']].plot(kind='bar', ax=plt.gca())\n",
    "            plt.title(f\"{level.capitalize()} Difficulty - Standard vs. Augmented\")\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.ylim(0, 1.1)\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Compare standard vs. augmented results\n",
    "compare_standard_vs_augmented(metrics, augmented_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Error Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_patterns(df, category, level):\n",
    "    \"\"\"Analyze error patterns for a specific category and difficulty level.\"\"\"\n",
    "    print(f\"Analyzing error patterns for {category} - {level} difficulty...\")\n",
    "    \n",
    "    # Define prediction column\n",
    "    prediction_col = f'multiple_choice_prediction_{level}'\n",
    "    \n",
    "    if prediction_col not in df.columns:\n",
    "        print(f\"Prediction column '{prediction_col}' not found.\")\n",
    "        return\n",
    "    \n",
    "    # Identify incorrect predictions\n",
    "    df_errors = df[df[prediction_col] != df['multiple_choice_solution']].copy()\n",
    "    \n",
    "    if len(df_errors) == 0:\n",
    "        print(\"No errors found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(df_errors)} errors out of {len(df)} samples ({len(df_errors)/len(df):.2%}).\")\n",
    "    \n",
    "    # Analyze error distribution\n",
    "    error_distribution = df_errors.groupby(['multiple_choice_solution', prediction_col]).size().unstack(fill_value=0)\n",
    "    print(\"\\nError distribution:\")\n",
    "    display(error_distribution)\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(error_distribution, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{category.capitalize()} - {level.capitalize()} Difficulty - Error Distribution\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return a few examples of errors\n",
    "    print(\"\\nExamples of errors:\")\n",
    "    display(df_errors.head(3))\n",
    "\n",
    "# Analyze error patterns for each category and difficulty level\n",
    "for category, df in result_data.items():\n",
    "    if df is not None:\n",
    "        for level in ['easy', 'medium', 'hard']:\n",
    "            analyze_error_patterns(df, category, level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has analyzed the results of the VQA4Mix project, including calculating accuracy metrics, visualizing results, and comparing performance across categories and difficulty levels. The analysis provides insights into the performance of the model on different types of images and the impact of image augmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
