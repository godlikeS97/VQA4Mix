{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88aeb58f",
   "metadata": {},
   "source": [
    "## Load Food Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43dd2d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228915</td>\n",
       "      <td>COCO_train2014_000000228915.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg</td>\n",
       "      <td>[A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>280866</td>\n",
       "      <td>COCO_train2014_000000280866.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000280866.jpg</td>\n",
       "      <td>[A person crossing their legs while a cat walks against a wall., a person wearing red tennis shoes and a black cat, Person with red tennis shoes with feet up near black cat., A person's feet in red shoes next to a black cat., A black cat walking a ledge in a dark room.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>467333</td>\n",
       "      <td>COCO_train2014_000000467333.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000467333.jpg</td>\n",
       "      <td>[A fluffy cat laying on the arm of a couch., A black and white photo of a cat with a remote control., A cat is laying on the couch arm by a remote control., A cat is sitting on the arm of a chair with a remote., A cat sitting on a couch arm next to a remote control.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>476960</td>\n",
       "      <td>COCO_train2014_000000476960.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000476960.jpg</td>\n",
       "      <td>[A cat standing with a banana in its mouth., There is a cat that is trying to eat a banana, a house cat playing with a ripe banana, A cat with a banana in its mouth., The cat has the banana in his mouth.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118960</td>\n",
       "      <td>COCO_train2014_000000118960.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000118960.jpg</td>\n",
       "      <td>[A couple of men standing next to each other near a parking lot., Two men standing near a parked car having a conversation., Some guys standing on a side walk by a car., Two people standing on the sidewalk, one with a backpack that has a cat inside it., Two men have a conversation on a sidewalk.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                        file_name  \\\n",
       "0  228915  COCO_train2014_000000228915.jpg   \n",
       "1  280866  COCO_train2014_000000280866.jpg   \n",
       "2  467333  COCO_train2014_000000467333.jpg   \n",
       "3  476960  COCO_train2014_000000476960.jpg   \n",
       "4  118960  COCO_train2014_000000118960.jpg   \n",
       "\n",
       "                                                                       url  \\\n",
       "0  http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg   \n",
       "1  http://images.cocodataset.org/train2014/COCO_train2014_000000280866.jpg   \n",
       "2  http://images.cocodataset.org/train2014/COCO_train2014_000000467333.jpg   \n",
       "3  http://images.cocodataset.org/train2014/COCO_train2014_000000476960.jpg   \n",
       "4  http://images.cocodataset.org/train2014/COCO_train2014_000000118960.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    captions  \n",
       "0                              [A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]  \n",
       "1                             [A person crossing their legs while a cat walks against a wall., a person wearing red tennis shoes and a black cat, Person with red tennis shoes with feet up near black cat., A person's feet in red shoes next to a black cat., A black cat walking a ledge in a dark room.]  \n",
       "2                                [A fluffy cat laying on the arm of a couch., A black and white photo of a cat with a remote control., A cat is laying on the couch arm by a remote control., A cat is sitting on the arm of a chair with a remote., A cat sitting on a couch arm next to a remote control.]  \n",
       "3                                                                                               [A cat standing with a banana in its mouth., There is a cat that is trying to eat a banana, a house cat playing with a ripe banana, A cat with a banana in its mouth., The cat has the banana in his mouth.]  \n",
       "4  [A couple of men standing next to each other near a parking lot., Two men standing near a parked car having a conversation., Some guys standing on a side walk by a car., Two people standing on the sidewalk, one with a backpack that has a cat inside it., Two men have a conversation on a sidewalk.]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shamelessly stolen from groupmates\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import json\n",
    "\n",
    "\n",
    "# Data Directory: \n",
    "# upking_images_directory = '/shared/data/upking/'\n",
    "upking_annotation_file_path = '/shared/data/upking/cat_data.json'\n",
    "\n",
    "\n",
    "# Method 1: Using pandas.read_json directly\n",
    "df = pd.read_json(upking_annotation_file_path)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0458394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a646505",
   "metadata": {},
   "source": [
    "## Generate Multiple Choice Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ad9b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random choice in [A, B, C, D]\n",
    "import random\n",
    "\n",
    "def generate_random_choice():\n",
    "    return random.choice(['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1ac2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['multiple_choice_solution'] = df.apply(lambda x: generate_random_choice(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4658c508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>captions</th>\n",
       "      <th>multiple_choice_solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>260978</td>\n",
       "      <td>COCO_train2014_000000260978.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000260978.jpg</td>\n",
       "      <td>[A cat is rolling around on a couch, An orange cat lying on its side on a bed., A cat laying on a couch with a sheet and a remote on it., An orange cat on a sofa with a remote, A cat upside on the couch near a remote]</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>142601</td>\n",
       "      <td>COCO_train2014_000000142601.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000142601.jpg</td>\n",
       "      <td>[One black cat and one orange cat sleeping on a bed near pillows., Two cats curled up on opposite sides of a bed., two cats one black the other a light brown sleeping on a bed, A couple of cats are laying on a large bed., Cats sleeping on a quilted bed in a room.]</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>436014</td>\n",
       "      <td>COCO_train2014_000000436014.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000436014.jpg</td>\n",
       "      <td>[A man using a laptop with cats sitting next to him on the table., Cats sit on a desk while a young man uses a laptop., A man on a laptop while two cats sit with him., A man uses his laptop while cats sit around his table., A man works on a computer white cats sit on the table.]</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>103447</td>\n",
       "      <td>COCO_train2014_000000103447.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000103447.jpg</td>\n",
       "      <td>[A cat standing in a open suitcase on some cloths., A cat that is resting in a full bag of luggage., a cat lays inside of a suitcase with clothes in it , A cat standing in a laundry hamper looking down., An animal sitting in a suit case filled with clothing.]</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>372759</td>\n",
       "      <td>COCO_train2014_000000372759.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000372759.jpg</td>\n",
       "      <td>[a black cat is hiding in a box with shoes, A cat and some shoes side by side., There is a cat sitting next to a pair of boots., A cat sits among a pile of shoes in a confined space., A black cat peers at a bunch of black shoes.]</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                        file_name  \\\n",
       "15  260978  COCO_train2014_000000260978.jpg   \n",
       "16  142601  COCO_train2014_000000142601.jpg   \n",
       "17  436014  COCO_train2014_000000436014.jpg   \n",
       "18  103447  COCO_train2014_000000103447.jpg   \n",
       "19  372759  COCO_train2014_000000372759.jpg   \n",
       "\n",
       "                                                                        url  \\\n",
       "15  http://images.cocodataset.org/train2014/COCO_train2014_000000260978.jpg   \n",
       "16  http://images.cocodataset.org/train2014/COCO_train2014_000000142601.jpg   \n",
       "17  http://images.cocodataset.org/train2014/COCO_train2014_000000436014.jpg   \n",
       "18  http://images.cocodataset.org/train2014/COCO_train2014_000000103447.jpg   \n",
       "19  http://images.cocodataset.org/train2014/COCO_train2014_000000372759.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                   captions  \\\n",
       "15                                                                [A cat is rolling around on a couch, An orange cat lying on its side on a bed., A cat laying on a couch with a sheet and a remote on it., An orange cat on a sofa with a remote, A cat upside on the couch near a remote]   \n",
       "16                 [One black cat and one orange cat sleeping on a bed near pillows., Two cats curled up on opposite sides of a bed., two cats one black the other a light brown sleeping on a bed, A couple of cats are laying on a large bed., Cats sleeping on a quilted bed in a room.]   \n",
       "17  [A man using a laptop with cats sitting next to him on the table., Cats sit on a desk while a young man uses a laptop., A man on a laptop while two cats sit with him., A man uses his laptop while cats sit around his table., A man works on a computer white cats sit on the table.]   \n",
       "18                      [A cat standing in a open suitcase on some cloths., A cat that is resting in a full bag of luggage., a cat lays inside of a suitcase with clothes in it , A cat standing in a laundry hamper looking down., An animal sitting in a suit case filled with clothing.]   \n",
       "19                                                    [a black cat is hiding in a box with shoes, A cat and some shoes side by side., There is a cat sitting next to a pair of boots., A cat sits among a pile of shoes in a confined space., A black cat peers at a bunch of black shoes.]   \n",
       "\n",
       "   multiple_choice_solution  \n",
       "15                        C  \n",
       "16                        A  \n",
       "17                        D  \n",
       "18                        B  \n",
       "19                        A  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple choice questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca90e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    ")\n",
    "\n",
    "\n",
    "def generate_multiple_choice_question(reference_caption, correct_choice): \n",
    "    # Define the prompt to generate inferior choices\n",
    "    prompt = f\"\"\"\n",
    "    The ground truth caption is:\n",
    "    \"{reference_caption}\"\n",
    "\n",
    "    Generate three inferior captions that include either inaccurate details, or are non-fluent with syntactic errors. \n",
    "    Format the result as a multiple-choice question. \n",
    "    Question title should be \"Which of the following captions best describes the painting?\".\n",
    "    The correct choice should be placed at choice \"{correct_choice}\". \n",
    "    Ensure the incorrect choices are realistic but clearly wrong.\n",
    "    Do not generate special symbols such as '*'.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=200,\n",
    "    )\n",
    "\n",
    "    # Extract the generated multiple-choice question\n",
    "    question = response.choices[0].message.content    \n",
    "    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e47d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lady wearing a ring and two bracelets holds a gray cat. A\n",
      "A person crossing their legs while a cat walks against a wall. C\n",
      "A fluffy cat laying on the arm of a couch. D\n",
      "A cat standing with a banana in its mouth. C\n",
      "A couple of men standing next to each other near a parking lot. D\n",
      "A multi-colored cat laying on a rug next to some bottles and a glass door. B\n",
      "A cat sitting in a sink over a cup of something. C\n",
      "A cat sits on a wooden chair with a purse. C\n",
      "A cat is hiding under a blanket and peering out. A\n",
      "A cat is laying on top of a laptop computer. A\n",
      "A dog and a cat stand side by side D\n",
      "Someone standing in a room with their shoes on their feet.  D\n",
      "there is a black tuxedo cat looking in the mirror B\n",
      "Two cats play with each other near some computers. B\n",
      "One cat plays with a banana while another lies down on a rug. C\n",
      "A cat is rolling around on a couch C\n",
      "One black cat and one orange cat sleeping on a bed near pillows. A\n",
      "A man using a laptop with cats sitting next to him on the table. D\n",
      "A cat standing in a open suitcase on some cloths. B\n",
      "a black cat is hiding in a box with shoes A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "19    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: print(x['captions'][0], x['multiple_choice_solution']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25b87e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['multiple_choice_question'] = df.apply(lambda x: generate_multiple_choice_question(x['captions'][0], x['multiple_choice_solution']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ca74bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>captions</th>\n",
       "      <th>multiple_choice_solution</th>\n",
       "      <th>multiple_choice_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228915</td>\n",
       "      <td>COCO_train2014_000000228915.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg</td>\n",
       "      <td>[A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]</td>\n",
       "      <td>A</td>\n",
       "      <td>**Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>280866</td>\n",
       "      <td>COCO_train2014_000000280866.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000280866.jpg</td>\n",
       "      <td>[A person crossing their legs while a cat walks against a wall., a person wearing red tennis shoes and a black cat, Person with red tennis shoes with feet up near black cat., A person's feet in red shoes next to a black cat., A black cat walking a ledge in a dark room.]</td>\n",
       "      <td>C</td>\n",
       "      <td>Which of the following captions best describes the painting?\\n\\nA) A dog runs across the field while a person jumps over a fence.\\n\\nB) Two cats are playing in a sunny meadow with no people around.\\n\\nC) A person crossing their legs while a cat walks against a wall.\\n\\nD) A bird flies over a pond with ducks swimming nearby.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>467333</td>\n",
       "      <td>COCO_train2014_000000467333.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000467333.jpg</td>\n",
       "      <td>[A fluffy cat laying on the arm of a couch., A black and white photo of a cat with a remote control., A cat is laying on the couch arm by a remote control., A cat is sitting on the arm of a chair with a remote., A cat sitting on a couch arm next to a remote control.]</td>\n",
       "      <td>D</td>\n",
       "      <td>Which of the following captions best describes the painting?\\n\\nA) A tiny dog sitting on a chair in the garden.\\n\\nB) A fluffy cat sleeping under the table.\\n\\nC) A fluffy cat laying on the sofa's pillow.\\n\\nD) A fluffy cat laying on the arm of a couch.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>476960</td>\n",
       "      <td>COCO_train2014_000000476960.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000476960.jpg</td>\n",
       "      <td>[A cat standing with a banana in its mouth., There is a cat that is trying to eat a banana, a house cat playing with a ripe banana, A cat with a banana in its mouth., The cat has the banana in his mouth.]</td>\n",
       "      <td>C</td>\n",
       "      <td>Which of the following captions best describes the painting?\\n\\nA) A cat sitting with an apple in its paw.\\n\\nB) A cat stand with bananas on its head.\\n\\nC) A cat standing with a banana in its mouth.\\n\\nD) A dog standing with a banana in its mouth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118960</td>\n",
       "      <td>COCO_train2014_000000118960.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000118960.jpg</td>\n",
       "      <td>[A couple of men standing next to each other near a parking lot., Two men standing near a parked car having a conversation., Some guys standing on a side walk by a car., Two people standing on the sidewalk, one with a backpack that has a cat inside it., Two men have a conversation on a sidewalk.]</td>\n",
       "      <td>D</td>\n",
       "      <td>Which of the following captions best describes the painting?\\n\\nA) Two men are sitting on a bench in a crowded park.\\n\\nB) A group of women walking together through a busy street.\\n\\nC) A single man looking at cars in a remote parking lot.\\n\\nD) A couple of men standing next to each other near a parking lot.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                        file_name  \\\n",
       "0  228915  COCO_train2014_000000228915.jpg   \n",
       "1  280866  COCO_train2014_000000280866.jpg   \n",
       "2  467333  COCO_train2014_000000467333.jpg   \n",
       "3  476960  COCO_train2014_000000476960.jpg   \n",
       "4  118960  COCO_train2014_000000118960.jpg   \n",
       "\n",
       "                                                                       url  \\\n",
       "0  http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg   \n",
       "1  http://images.cocodataset.org/train2014/COCO_train2014_000000280866.jpg   \n",
       "2  http://images.cocodataset.org/train2014/COCO_train2014_000000467333.jpg   \n",
       "3  http://images.cocodataset.org/train2014/COCO_train2014_000000476960.jpg   \n",
       "4  http://images.cocodataset.org/train2014/COCO_train2014_000000118960.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    captions  \\\n",
       "0                              [A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]   \n",
       "1                             [A person crossing their legs while a cat walks against a wall., a person wearing red tennis shoes and a black cat, Person with red tennis shoes with feet up near black cat., A person's feet in red shoes next to a black cat., A black cat walking a ledge in a dark room.]   \n",
       "2                                [A fluffy cat laying on the arm of a couch., A black and white photo of a cat with a remote control., A cat is laying on the couch arm by a remote control., A cat is sitting on the arm of a chair with a remote., A cat sitting on a couch arm next to a remote control.]   \n",
       "3                                                                                               [A cat standing with a banana in its mouth., There is a cat that is trying to eat a banana, a house cat playing with a ripe banana, A cat with a banana in its mouth., The cat has the banana in his mouth.]   \n",
       "4  [A couple of men standing next to each other near a parking lot., Two men standing near a parked car having a conversation., Some guys standing on a side walk by a car., Two people standing on the sidewalk, one with a backpack that has a cat inside it., Two men have a conversation on a sidewalk.]   \n",
       "\n",
       "  multiple_choice_solution  \\\n",
       "0                        A   \n",
       "1                        C   \n",
       "2                        D   \n",
       "3                        C   \n",
       "4                        D   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                multiple_choice_question  \n",
       "0                   **Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.  \n",
       "1  Which of the following captions best describes the painting?\\n\\nA) A dog runs across the field while a person jumps over a fence.\\n\\nB) Two cats are playing in a sunny meadow with no people around.\\n\\nC) A person crossing their legs while a cat walks against a wall.\\n\\nD) A bird flies over a pond with ducks swimming nearby.  \n",
       "2                                                                          Which of the following captions best describes the painting?\\n\\nA) A tiny dog sitting on a chair in the garden.\\n\\nB) A fluffy cat sleeping under the table.\\n\\nC) A fluffy cat laying on the sofa's pillow.\\n\\nD) A fluffy cat laying on the arm of a couch.  \n",
       "3                                                                               Which of the following captions best describes the painting?\\n\\nA) A cat sitting with an apple in its paw.\\n\\nB) A cat stand with bananas on its head.\\n\\nC) A cat standing with a banana in its mouth.\\n\\nD) A dog standing with a banana in its mouth.  \n",
       "4                  Which of the following captions best describes the painting?\\n\\nA) Two men are sitting on a bench in a crowded park.\\n\\nB) A group of women walking together through a busy street.\\n\\nC) A single man looking at cars in a remote parking lot.\\n\\nD) A couple of men standing next to each other near a parking lot.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "057e5631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as a list of dictionaries in /shared/data/upking/upking_annotation_with_MCQ.json\n"
     ]
    }
   ],
   "source": [
    "# Save the annotation with multiple choice question to output file\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "list_of_dicts = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Save the list of dictionaries to a JSON file\n",
    "output_file = \"/shared/data/upking/upking_annotation_with_MCQ.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(list_of_dicts, file, indent=4)\n",
    "\n",
    "print(f\"DataFrame saved as a list of dictionaries in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ef648",
   "metadata": {},
   "source": [
    "## Perform Multiple Choice Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002b381b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>captions</th>\n",
       "      <th>multiple_choice_solution</th>\n",
       "      <th>multiple_choice_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228915</td>\n",
       "      <td>COCO_train2014_000000228915.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg</td>\n",
       "      <td>[A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]</td>\n",
       "      <td>A</td>\n",
       "      <td>**Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>280866</td>\n",
       "      <td>COCO_train2014_000000280866.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000280866.jpg</td>\n",
       "      <td>[A person crossing their legs while a cat walks against a wall., a person wearing red tennis shoes and a black cat, Person with red tennis shoes with feet up near black cat., A person's feet in red shoes next to a black cat., A black cat walking a ledge in a dark room.]</td>\n",
       "      <td>C</td>\n",
       "      <td>Which of the following captions best describes the painting?\\n\\nA) A dog runs across the field while a person jumps over a fence.\\n\\nB) Two cats are playing in a sunny meadow with no people around.\\n\\nC) A person crossing their legs while a cat walks against a wall.\\n\\nD) A bird flies over a pond with ducks swimming nearby.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>467333</td>\n",
       "      <td>COCO_train2014_000000467333.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000467333.jpg</td>\n",
       "      <td>[A fluffy cat laying on the arm of a couch., A black and white photo of a cat with a remote control., A cat is laying on the couch arm by a remote control., A cat is sitting on the arm of a chair with a remote., A cat sitting on a couch arm next to a remote control.]</td>\n",
       "      <td>D</td>\n",
       "      <td>Which of the following captions best describes the painting?\\n\\nA) A tiny dog sitting on a chair in the garden.\\n\\nB) A fluffy cat sleeping under the table.\\n\\nC) A fluffy cat laying on the sofa's pillow.\\n\\nD) A fluffy cat laying on the arm of a couch.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>476960</td>\n",
       "      <td>COCO_train2014_000000476960.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000476960.jpg</td>\n",
       "      <td>[A cat standing with a banana in its mouth., There is a cat that is trying to eat a banana, a house cat playing with a ripe banana, A cat with a banana in its mouth., The cat has the banana in his mouth.]</td>\n",
       "      <td>C</td>\n",
       "      <td>Which of the following captions best describes the painting?\\n\\nA) A cat sitting with an apple in its paw.\\n\\nB) A cat stand with bananas on its head.\\n\\nC) A cat standing with a banana in its mouth.\\n\\nD) A dog standing with a banana in its mouth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118960</td>\n",
       "      <td>COCO_train2014_000000118960.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000118960.jpg</td>\n",
       "      <td>[A couple of men standing next to each other near a parking lot., Two men standing near a parked car having a conversation., Some guys standing on a side walk by a car., Two people standing on the sidewalk, one with a backpack that has a cat inside it., Two men have a conversation on a sidewalk.]</td>\n",
       "      <td>D</td>\n",
       "      <td>Which of the following captions best describes the painting?\\n\\nA) Two men are sitting on a bench in a crowded park.\\n\\nB) A group of women walking together through a busy street.\\n\\nC) A single man looking at cars in a remote parking lot.\\n\\nD) A couple of men standing next to each other near a parking lot.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                        file_name  \\\n",
       "0  228915  COCO_train2014_000000228915.jpg   \n",
       "1  280866  COCO_train2014_000000280866.jpg   \n",
       "2  467333  COCO_train2014_000000467333.jpg   \n",
       "3  476960  COCO_train2014_000000476960.jpg   \n",
       "4  118960  COCO_train2014_000000118960.jpg   \n",
       "\n",
       "                                                                       url  \\\n",
       "0  http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg   \n",
       "1  http://images.cocodataset.org/train2014/COCO_train2014_000000280866.jpg   \n",
       "2  http://images.cocodataset.org/train2014/COCO_train2014_000000467333.jpg   \n",
       "3  http://images.cocodataset.org/train2014/COCO_train2014_000000476960.jpg   \n",
       "4  http://images.cocodataset.org/train2014/COCO_train2014_000000118960.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    captions  \\\n",
       "0                              [A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]   \n",
       "1                             [A person crossing their legs while a cat walks against a wall., a person wearing red tennis shoes and a black cat, Person with red tennis shoes with feet up near black cat., A person's feet in red shoes next to a black cat., A black cat walking a ledge in a dark room.]   \n",
       "2                                [A fluffy cat laying on the arm of a couch., A black and white photo of a cat with a remote control., A cat is laying on the couch arm by a remote control., A cat is sitting on the arm of a chair with a remote., A cat sitting on a couch arm next to a remote control.]   \n",
       "3                                                                                               [A cat standing with a banana in its mouth., There is a cat that is trying to eat a banana, a house cat playing with a ripe banana, A cat with a banana in its mouth., The cat has the banana in his mouth.]   \n",
       "4  [A couple of men standing next to each other near a parking lot., Two men standing near a parked car having a conversation., Some guys standing on a side walk by a car., Two people standing on the sidewalk, one with a backpack that has a cat inside it., Two men have a conversation on a sidewalk.]   \n",
       "\n",
       "  multiple_choice_solution  \\\n",
       "0                        A   \n",
       "1                        C   \n",
       "2                        D   \n",
       "3                        C   \n",
       "4                        D   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                multiple_choice_question  \n",
       "0                   **Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.  \n",
       "1  Which of the following captions best describes the painting?\\n\\nA) A dog runs across the field while a person jumps over a fence.\\n\\nB) Two cats are playing in a sunny meadow with no people around.\\n\\nC) A person crossing their legs while a cat walks against a wall.\\n\\nD) A bird flies over a pond with ducks swimming nearby.  \n",
       "2                                                                          Which of the following captions best describes the painting?\\n\\nA) A tiny dog sitting on a chair in the garden.\\n\\nB) A fluffy cat sleeping under the table.\\n\\nC) A fluffy cat laying on the sofa's pillow.\\n\\nD) A fluffy cat laying on the arm of a couch.  \n",
       "3                                                                               Which of the following captions best describes the painting?\\n\\nA) A cat sitting with an apple in its paw.\\n\\nB) A cat stand with bananas on its head.\\n\\nC) A cat standing with a banana in its mouth.\\n\\nD) A dog standing with a banana in its mouth.  \n",
       "4                  Which of the following captions best describes the painting?\\n\\nA) Two men are sitting on a bench in a crowded park.\\n\\nB) A group of women walking together through a busy street.\\n\\nC) A single man looking at cars in a remote parking lot.\\n\\nD) A couple of men standing next to each other near a parking lot.  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load annotation with multiple choice question data file\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import json\n",
    "\n",
    "\n",
    "# Data Directory: \n",
    "#food_images_directory = '/shared/data/food_data/food_images/'\n",
    "upking_MCQ_file_path = '/shared/data/upking/upking_annotation_with_MCQ.json'\n",
    "\n",
    "\n",
    "# Method 1: Using pandas.read_json directly\n",
    "df = pd.read_json(upking_MCQ_file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cbb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f04f522e",
   "metadata": {},
   "source": [
    "### Use llava model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839664b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae119ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bed81441db4c3598072a44408591b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model from local directory \n",
    "model_path = '/shared/model/llava-v1.6-mistral-7b-hf'\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_path)\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, load_in_4bit=True) \n",
    "#model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "426c1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "121f75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: \n",
    "def perform_multiple_choice_task_llava(img_url, question):\n",
    "    image = io.imread(img_url)\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": question + \"\\nOnly return the correct choice with a single letter.\"},\n",
    "              {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "    # autoregressively complete prompt\n",
    "    output = model.generate(**inputs, max_new_tokens=150)\n",
    "    output = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # mcq_question = output.split('[/INST]')[0].split('[INST] ')[1].strip()\n",
    "    mcq_answer = output.split('[/INST]')[1].strip()\n",
    "    # del image, inputs, output\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    return mcq_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f2af250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_batches(df, batch_size):\n",
    "    predictions = []  # 用于存储结果\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        # 取出当前批次的数据\n",
    "        batch = df.iloc[start:start + batch_size]\n",
    "        batch_predictions = []\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            # 对每一行执行多选任务\n",
    "            prediction = perform_multiple_choice_task_llava(row['url'], row['multiple_choice_question'])\n",
    "            batch_predictions.append(prediction)\n",
    "        \n",
    "        # 将当前批次的结果添加到最终列表中\n",
    "        predictions.extend(batch_predictions)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5c5c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.57 GiB of which 6.75 MiB is free. Process 1034175 has 9.60 GiB memory in use. Including non-PyTorch memory, this process has 4.96 GiB memory in use. Of the allocated memory 4.79 GiB is allocated by PyTorch, and 33.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 执行批量处理，并将预测结果添加为新的列\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiple_choice_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 10\u001b[0m, in \u001b[0;36mprocess_in_batches\u001b[0;34m(df, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# 对每一行执行多选任务\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mperform_multiple_choice_task_llava\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmultiple_choice_question\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     batch_predictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 将当前批次的结果添加到最终列表中\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 20\u001b[0m, in \u001b[0;36mperform_multiple_choice_task_llava\u001b[0;34m(img_url, question)\u001b[0m\n\u001b[1;32m     17\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, text\u001b[38;5;241m=\u001b[39mprompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# autoregressively complete prompt\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m output \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# mcq_question = output.split('[/INST]')[0].split('[INST] ')[1].strip()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llava_next/modeling_llava_next.py:851\u001b[0m, in \u001b[0;36mLlavaNextForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, image_sizes, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m    849\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 851\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_feature_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_feature_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_feature_select_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# NOTE we only support multimodal_patch_merge_type == \"spatial_unpad\"\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     image_features, feature_lens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_image_features(\n\u001b[1;32m    860\u001b[0m         image_features,\n\u001b[1;32m    861\u001b[0m         image_sizes,\n\u001b[1;32m    862\u001b[0m         vision_feature_select_strategy\u001b[38;5;241m=\u001b[39mvision_feature_select_strategy,\n\u001b[1;32m    863\u001b[0m         image_newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_newline,\n\u001b[1;32m    864\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llava_next/modeling_llava_next.py:749\u001b[0m, in \u001b[0;36mLlavaNextForConditionalGeneration.get_image_features\u001b[0;34m(self, pixel_values, image_sizes, vision_feature_layer, vision_feature_select_strategy)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;66;03m# otherwise has to be stacked from list of (num_patches, num_channels, height, width)\u001b[39;00m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixel_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expect to be of 4 or 5 dimensions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 749\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m selected_image_feature \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mhidden_states[vision_feature_layer]\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vision_feature_select_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1171\u001b[0m, in \u001b[0;36mCLIPVisionModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled CLS states\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1097\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1094\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[1;32m   1095\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m-> 1097\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1105\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:877\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    869\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    870\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    871\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    874\u001b[0m         output_attentions,\n\u001b[1;32m    875\u001b[0m     )\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:608\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    605\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    607\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 608\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    616\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:330\u001b[0m, in \u001b[0;36mCLIPAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m bsz, tgt_len, embed_dim \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    331\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[1;32m    332\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:486\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    483\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    484\u001b[0m out \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mmatmul_4bit(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt(), bias\u001b[38;5;241m=\u001b[39mbias, quant_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mquant_state)\n\u001b[0;32m--> 486\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.57 GiB of which 6.75 MiB is free. Process 1034175 has 9.60 GiB memory in use. Including non-PyTorch memory, this process has 4.96 GiB memory in use. Of the allocated memory 4.79 GiB is allocated by PyTorch, and 33.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 设置批量大小（根据显存容量调整）\n",
    "batch_size = 1\n",
    "\n",
    "# 执行批量处理，并将预测结果添加为新的列\n",
    "df['multiple_choice_prediction'] = process_in_batches(df, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f8b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "df['multiple_choice_prediction'] = df.apply(lambda x: perform_multiple_choice_task_llava(x['url'], x['multiple_choice_question']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc0669fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>captions</th>\n",
       "      <th>multiple_choice_solution</th>\n",
       "      <th>multiple_choice_question</th>\n",
       "      <th>multiple_choice_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228915</td>\n",
       "      <td>COCO_train2014_000000228915.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg</td>\n",
       "      <td>[A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]</td>\n",
       "      <td>A</td>\n",
       "      <td>**Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                        file_name  \\\n",
       "0  228915  COCO_train2014_000000228915.jpg   \n",
       "\n",
       "                                                                       url  \\\n",
       "0  http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                        captions  \\\n",
       "0  [A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]   \n",
       "\n",
       "  multiple_choice_solution  \\\n",
       "0                        A   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                               multiple_choice_question  \\\n",
       "0  **Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.   \n",
       "\n",
       "  multiple_choice_prediction  \n",
       "0                          A  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1663b700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>multiple_choice_solution</th>\n",
       "      <th>multiple_choice_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   multiple_choice_solution multiple_choice_prediction\n",
       "0                         A                          A\n",
       "1                         C                          C\n",
       "2                         D                          C\n",
       "3                         C                          C\n",
       "4                         D                          D\n",
       "5                         B                          B\n",
       "6                         C                          C\n",
       "7                         C                          C\n",
       "8                         A                          A\n",
       "9                         A                          A\n",
       "10                        D                          D\n",
       "11                        D                          B\n",
       "12                        B                          B\n",
       "13                        B                          A\n",
       "14                        C                          C\n",
       "15                        C                          C\n",
       "16                        A                          A\n",
       "17                        D                          D\n",
       "18                        B                          B\n",
       "19                        A                          A"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['multiple_choice_solution', 'multiple_choice_prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00ef517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as a list of dictionaries in /shared/data/upking/upking_annotation_with_MCQ_result.json\n"
     ]
    }
   ],
   "source": [
    "# Save the MCQ result\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "list_of_dicts = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Save the list of dictionaries to a JSON file\n",
    "output_file = \"/shared/data/upking/upking_annotation_with_MCQ_result.json\"\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(list_of_dicts, file, indent=4)\n",
    "\n",
    "print(f\"DataFrame saved as a list of dictionaries in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f58338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e33e34b",
   "metadata": {},
   "source": [
    "## Perform Image Captioning Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc753d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>captions</th>\n",
       "      <th>multiple_choice_solution</th>\n",
       "      <th>multiple_choice_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228915</td>\n",
       "      <td>COCO_train2014_000000228915.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg</td>\n",
       "      <td>[A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]</td>\n",
       "      <td>A</td>\n",
       "      <td>**Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                        file_name  \\\n",
       "0  228915  COCO_train2014_000000228915.jpg   \n",
       "\n",
       "                                                                       url  \\\n",
       "0  http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                        captions  \\\n",
       "0  [A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]   \n",
       "\n",
       "  multiple_choice_solution  \\\n",
       "0                        A   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                               multiple_choice_question  \n",
       "0  **Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load annotation with multiple choice question data file\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import json\n",
    "\n",
    "\n",
    "# Data Directory: \n",
    "#food_images_directory = '/shared/data/food_data/food_images/'\n",
    "food_annotation_file_path = '/shared/data/upking/upking_annotation_with_MCQ.json'\n",
    "\n",
    "\n",
    "# Method 1: Using pandas.read_json directly\n",
    "df = pd.read_json(food_annotation_file_path)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b51ee0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: \n",
    "def perform_image_captioning_task_llava(img_url):\n",
    "    image = io.imread(img_url)\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": \"Generate a caption for this image.\"},\n",
    "              {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "    # autoregressively complete prompt\n",
    "    output = model.generate(**inputs, max_new_tokens=150)\n",
    "    output = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    caption = output.split('[/INST]')[1].strip()\n",
    "    print(caption)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "510030c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A moment of affection between a human and their feline friend.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy evening with a feline friend.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy moment captured in black and white.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A curious feline explores the world of human snacks.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Friends sharing a moment on the sidewalk.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A curious feline gazes out the window, surrounded by the warmth of a cozy home.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy cat enjoying a warm beverage.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A curious feline observing the world from its perch.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy hideout for a curious feline.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy afternoon nap with a laptop as a pillow.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A moment of tranquility between two feline friends.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A moment of tranquility shared between a human and their feline companion.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A curious feline takes a selfie in the mirror.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy corner with a cat, a mouse, and a TV.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy scene of two feline friends sharing a playful moment on a vibrant rug.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy evening with a feline friend.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy corner with two feline friends sharing a bed.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A cozy workspace with feline companions.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A curious feline exploring the depths of a suitcase.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A curious feline explores a collection of shoes and boots.\"\n"
     ]
    }
   ],
   "source": [
    "df['predicted_caption'] = df.apply(lambda x: perform_image_captioning_task_llava(x['url']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc9f48f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>captions</th>\n",
       "      <th>predicted_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]</td>\n",
       "      <td>\"A moment of affection between a human and their feline friend.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[A person crossing their legs while a cat walks against a wall., a person wearing red tennis shoes and a black cat, Person with red tennis shoes with feet up near black cat., A person's feet in red shoes next to a black cat., A black cat walking a ledge in a dark room.]</td>\n",
       "      <td>\"A cozy evening with a feline friend.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[A fluffy cat laying on the arm of a couch., A black and white photo of a cat with a remote control., A cat is laying on the couch arm by a remote control., A cat is sitting on the arm of a chair with a remote., A cat sitting on a couch arm next to a remote control.]</td>\n",
       "      <td>\"A cozy moment captured in black and white.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[A cat standing with a banana in its mouth., There is a cat that is trying to eat a banana, a house cat playing with a ripe banana, A cat with a banana in its mouth., The cat has the banana in his mouth.]</td>\n",
       "      <td>\"A curious feline explores the world of human snacks.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[A couple of men standing next to each other near a parking lot., Two men standing near a parked car having a conversation., Some guys standing on a side walk by a car., Two people standing on the sidewalk, one with a backpack that has a cat inside it., Two men have a conversation on a sidewalk.]</td>\n",
       "      <td>\"Friends sharing a moment on the sidewalk.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[A multi-colored cat laying on a rug next to some bottles and a glass door., A cat sitting on the floor looking up, The cat looks upward beside some glass bottles., A very cute cat on a rug by some bottles., Multi colored cat laying on the floor next to door and liquor bottles]</td>\n",
       "      <td>\"A curious feline gazes out the window, surrounded by the warmth of a cozy home.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[A cat sitting in a sink over a cup of something., A tortoiseshell cat stands in a stainless steel sink and drinks from a cup., A cat looking up while sipping from a cup in a sink., A very cute cat sitting in a sink drinking from a cup., furry cat drinking from a paper cup inside a kitchen sink]</td>\n",
       "      <td>\"A cozy cat enjoying a warm beverage.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[A cat sits on a wooden chair with a purse., A cat sitting on a wooden chair next to table., A cat sitting on a chair in a restaurant, A cat sits on a cafe chair with a handbag, Grey and white cat sitting on top of a wooden chair. ]</td>\n",
       "      <td>\"A curious feline observing the world from its perch.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[A cat is hiding under a blanket and peering out., A black cat hiding under a blue blanket., A cat looks out from under a blue blanket., There is a cat lying under a blanket, a cat looking out from underneath a blanket.]</td>\n",
       "      <td>\"A cozy hideout for a curious feline.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[A cat is laying on top of a laptop computer., A cat taking a nap on a laptop., A ginger cat resting on a PC laptop, A brown cat napping on a laptop kept on a sofa., a cat sleeps on a laptop computer on a couch]</td>\n",
       "      <td>\"A cozy afternoon nap with a laptop as a pillow.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[A dog and a cat stand side by side, An adorable cat sitting next to an adorable dog., A dog and cat sitting on the ground., a cat crouches menacingly along side a disinterested small dog, Small little dog standing next to a smaller mixed colored cat.]</td>\n",
       "      <td>\"A moment of tranquility between two feline friends.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Someone standing in a room with their shoes on their feet. , A man is wearing white tennis shoes on a tile floor., A striped cat stands at a person's feet on a tile floor., A man standing on a tiled floor with a cat next to him, The camera person is wearing white tennis shoes out on the patio.]</td>\n",
       "      <td>\"A moment of tranquility shared between a human and their feline companion.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[there is a black tuxedo cat looking in the mirror, Two cats sitting on top of a wooden floor., A cat looking at itself in the mirror next to a tripod., A cat and a tripod sitting in front of a mirror., a close up of a cat in a mirror]</td>\n",
       "      <td>\"A curious feline takes a selfie in the mirror.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Two cats play with each other near some computers., Two mice, monitor screen and cat on the table, A cat playing around on a computer desk near a TV and laptop. , A cat staring at the mouse for a computer., A cat is climbing on a book shelf looking at things.]</td>\n",
       "      <td>\"A cozy corner with a cat, a mouse, and a TV.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[One cat plays with a banana while another lies down on a rug., A cat playing with a banana toy as another cat looks on., A pair of cats play on a rug together., A couple of cats are playing on a rug, Two cats laying on the floor playing with toys]</td>\n",
       "      <td>\"A cozy scene of two feline friends sharing a playful moment on a vibrant rug.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[A cat is rolling around on a couch, An orange cat lying on its side on a bed., A cat laying on a couch with a sheet and a remote on it., An orange cat on a sofa with a remote, A cat upside on the couch near a remote]</td>\n",
       "      <td>\"A cozy evening with a feline friend.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[One black cat and one orange cat sleeping on a bed near pillows., Two cats curled up on opposite sides of a bed., two cats one black the other a light brown sleeping on a bed, A couple of cats are laying on a large bed., Cats sleeping on a quilted bed in a room.]</td>\n",
       "      <td>\"A cozy corner with two feline friends sharing a bed.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[A man using a laptop with cats sitting next to him on the table., Cats sit on a desk while a young man uses a laptop., A man on a laptop while two cats sit with him., A man uses his laptop while cats sit around his table., A man works on a computer white cats sit on the table.]</td>\n",
       "      <td>\"A cozy workspace with feline companions.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[A cat standing in a open suitcase on some cloths., A cat that is resting in a full bag of luggage., a cat lays inside of a suitcase with clothes in it , A cat standing in a laundry hamper looking down., An animal sitting in a suit case filled with clothing.]</td>\n",
       "      <td>\"A curious feline exploring the depths of a suitcase.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[a black cat is hiding in a box with shoes, A cat and some shoes side by side., There is a cat sitting next to a pair of boots., A cat sits among a pile of shoes in a confined space., A black cat peers at a bunch of black shoes.]</td>\n",
       "      <td>\"A curious feline explores a collection of shoes and boots.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                     captions  \\\n",
       "0                               [A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]   \n",
       "1                              [A person crossing their legs while a cat walks against a wall., a person wearing red tennis shoes and a black cat, Person with red tennis shoes with feet up near black cat., A person's feet in red shoes next to a black cat., A black cat walking a ledge in a dark room.]   \n",
       "2                                 [A fluffy cat laying on the arm of a couch., A black and white photo of a cat with a remote control., A cat is laying on the couch arm by a remote control., A cat is sitting on the arm of a chair with a remote., A cat sitting on a couch arm next to a remote control.]   \n",
       "3                                                                                                [A cat standing with a banana in its mouth., There is a cat that is trying to eat a banana, a house cat playing with a ripe banana, A cat with a banana in its mouth., The cat has the banana in his mouth.]   \n",
       "4   [A couple of men standing next to each other near a parking lot., Two men standing near a parked car having a conversation., Some guys standing on a side walk by a car., Two people standing on the sidewalk, one with a backpack that has a cat inside it., Two men have a conversation on a sidewalk.]   \n",
       "5                      [A multi-colored cat laying on a rug next to some bottles and a glass door., A cat sitting on the floor looking up, The cat looks upward beside some glass bottles., A very cute cat on a rug by some bottles., Multi colored cat laying on the floor next to door and liquor bottles]   \n",
       "6    [A cat sitting in a sink over a cup of something., A tortoiseshell cat stands in a stainless steel sink and drinks from a cup., A cat looking up while sipping from a cup in a sink., A very cute cat sitting in a sink drinking from a cup., furry cat drinking from a paper cup inside a kitchen sink]   \n",
       "7                                                                    [A cat sits on a wooden chair with a purse., A cat sitting on a wooden chair next to table., A cat sitting on a chair in a restaurant, A cat sits on a cafe chair with a handbag, Grey and white cat sitting on top of a wooden chair. ]   \n",
       "8                                                                                [A cat is hiding under a blanket and peering out., A black cat hiding under a blue blanket., A cat looks out from under a blue blanket., There is a cat lying under a blanket, a cat looking out from underneath a blanket.]   \n",
       "9                                                                                         [A cat is laying on top of a laptop computer., A cat taking a nap on a laptop., A ginger cat resting on a PC laptop, A brown cat napping on a laptop kept on a sofa., a cat sleeps on a laptop computer on a couch]   \n",
       "10                                               [A dog and a cat stand side by side, An adorable cat sitting next to an adorable dog., A dog and cat sitting on the ground., a cat crouches menacingly along side a disinterested small dog, Small little dog standing next to a smaller mixed colored cat.]   \n",
       "11   [Someone standing in a room with their shoes on their feet. , A man is wearing white tennis shoes on a tile floor., A striped cat stands at a person's feet on a tile floor., A man standing on a tiled floor with a cat next to him, The camera person is wearing white tennis shoes out on the patio.]   \n",
       "12                                                                [there is a black tuxedo cat looking in the mirror, Two cats sitting on top of a wooden floor., A cat looking at itself in the mirror next to a tripod., A cat and a tripod sitting in front of a mirror., a close up of a cat in a mirror]   \n",
       "13                                      [Two cats play with each other near some computers., Two mice, monitor screen and cat on the table, A cat playing around on a computer desk near a TV and laptop. , A cat staring at the mouse for a computer., A cat is climbing on a book shelf looking at things.]   \n",
       "14                                                   [One cat plays with a banana while another lies down on a rug., A cat playing with a banana toy as another cat looks on., A pair of cats play on a rug together., A couple of cats are playing on a rug, Two cats laying on the floor playing with toys]   \n",
       "15                                                                                  [A cat is rolling around on a couch, An orange cat lying on its side on a bed., A cat laying on a couch with a sheet and a remote on it., An orange cat on a sofa with a remote, A cat upside on the couch near a remote]   \n",
       "16                                   [One black cat and one orange cat sleeping on a bed near pillows., Two cats curled up on opposite sides of a bed., two cats one black the other a light brown sleeping on a bed, A couple of cats are laying on a large bed., Cats sleeping on a quilted bed in a room.]   \n",
       "17                    [A man using a laptop with cats sitting next to him on the table., Cats sit on a desk while a young man uses a laptop., A man on a laptop while two cats sit with him., A man uses his laptop while cats sit around his table., A man works on a computer white cats sit on the table.]   \n",
       "18                                        [A cat standing in a open suitcase on some cloths., A cat that is resting in a full bag of luggage., a cat lays inside of a suitcase with clothes in it , A cat standing in a laundry hamper looking down., An animal sitting in a suit case filled with clothing.]   \n",
       "19                                                                      [a black cat is hiding in a box with shoes, A cat and some shoes side by side., There is a cat sitting next to a pair of boots., A cat sits among a pile of shoes in a confined space., A black cat peers at a bunch of black shoes.]   \n",
       "\n",
       "                                                                    predicted_caption  \n",
       "0                    \"A moment of affection between a human and their feline friend.\"  \n",
       "1                                              \"A cozy evening with a feline friend.\"  \n",
       "2                                        \"A cozy moment captured in black and white.\"  \n",
       "3                              \"A curious feline explores the world of human snacks.\"  \n",
       "4                                         \"Friends sharing a moment on the sidewalk.\"  \n",
       "5   \"A curious feline gazes out the window, surrounded by the warmth of a cozy home.\"  \n",
       "6                                              \"A cozy cat enjoying a warm beverage.\"  \n",
       "7                              \"A curious feline observing the world from its perch.\"  \n",
       "8                                              \"A cozy hideout for a curious feline.\"  \n",
       "9                                   \"A cozy afternoon nap with a laptop as a pillow.\"  \n",
       "10                              \"A moment of tranquility between two feline friends.\"  \n",
       "11       \"A moment of tranquility shared between a human and their feline companion.\"  \n",
       "12                                   \"A curious feline takes a selfie in the mirror.\"  \n",
       "13                                     \"A cozy corner with a cat, a mouse, and a TV.\"  \n",
       "14    \"A cozy scene of two feline friends sharing a playful moment on a vibrant rug.\"  \n",
       "15                                             \"A cozy evening with a feline friend.\"  \n",
       "16                             \"A cozy corner with two feline friends sharing a bed.\"  \n",
       "17                                         \"A cozy workspace with feline companions.\"  \n",
       "18                             \"A curious feline exploring the depths of a suitcase.\"  \n",
       "19                       \"A curious feline explores a collection of shoes and boots.\"  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['captions', 'predicted_caption']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0f3de53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as a list of dictionaries in /shared/data/upking/upking_annotation_with_image_captioning_result.json\n"
     ]
    }
   ],
   "source": [
    "# Save the image_captioning result\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "list_of_dicts = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Save the list of dictionaries to a JSON file\n",
    "output_file = \"/shared/data/upking/upking_annotation_with_image_captioning_result.json\"\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(list_of_dicts, file, indent=4)\n",
    "\n",
    "print(f\"DataFrame saved as a list of dictionaries in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e19d3d",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae28d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104932c",
   "metadata": {},
   "source": [
    "### Multiple Choice Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f93141d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>captions</th>\n",
       "      <th>multiple_choice_solution</th>\n",
       "      <th>multiple_choice_question</th>\n",
       "      <th>multiple_choice_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228915</td>\n",
       "      <td>COCO_train2014_000000228915.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg</td>\n",
       "      <td>[A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]</td>\n",
       "      <td>A</td>\n",
       "      <td>**Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                        file_name  \\\n",
       "0  228915  COCO_train2014_000000228915.jpg   \n",
       "\n",
       "                                                                       url  \\\n",
       "0  http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                        captions  \\\n",
       "0  [A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]   \n",
       "\n",
       "  multiple_choice_solution  \\\n",
       "0                        A   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                               multiple_choice_question  \\\n",
       "0  **Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.   \n",
       "\n",
       "  multiple_choice_prediction  \n",
       "0                          A  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load annotation with multiple choice question result data file\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import json\n",
    "\n",
    "\n",
    "# Data Directory: \n",
    "food_annotation_file_path = '/shared/data/upking/upking_annotation_with_MCQ_result.json'\n",
    "\n",
    "\n",
    "# Method 1: Using pandas.read_json directly\n",
    "df = pd.read_json(food_annotation_file_path)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c32a0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_multiple_choice_question_accuracy(df):\n",
    "    # Calculate accuracy\n",
    "    accuracy = (df[\"multiple_choice_solution\"] == df[\"multiple_choice_prediction\"]).mean()\n",
    "\n",
    "    print(f\"Prediction Accuracy: {accuracy * 100:.2f}%\") \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02acaf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 85.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_multiple_choice_question_accuracy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff5a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62a7399c",
   "metadata": {},
   "source": [
    "### Caption Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01e8d555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>captions</th>\n",
       "      <th>multiple_choice_solution</th>\n",
       "      <th>multiple_choice_question</th>\n",
       "      <th>predicted_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228915</td>\n",
       "      <td>COCO_train2014_000000228915.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg</td>\n",
       "      <td>[A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]</td>\n",
       "      <td>A</td>\n",
       "      <td>**Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.</td>\n",
       "      <td>\"A moment of affection between a human and their feline friend.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                        file_name  \\\n",
       "0  228915  COCO_train2014_000000228915.jpg   \n",
       "\n",
       "                                                                       url  \\\n",
       "0  http://images.cocodataset.org/train2014/COCO_train2014_000000228915.jpg   \n",
       "\n",
       "                                                                                                                                                                                                                                                                        captions  \\\n",
       "0  [A lady wearing a ring and two bracelets holds a gray cat., An uncomfortable looking cat wearing a tight white fabric collar, Someone holding a grey cat that has been shaved. , A dark gray cat with shaved fur is held by a woman., a cat with a collar sitting by a woman]   \n",
       "\n",
       "  multiple_choice_solution  \\\n",
       "0                        A   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                               multiple_choice_question  \\\n",
       "0  **Which of the following captions best describes the painting?**\\n\\nA) A lady wearing a ring and two bracelets holds a gray cat.\\n\\nB) A man with a watch and three rings holding a small dog.\\n\\nC) A woman wearing a necklace and a hat holds a brown cat.\\n\\nD) A lady holding gray cat with no jewelry on hands.   \n",
       "\n",
       "                                                  predicted_caption  \n",
       "0  \"A moment of affection between a human and their feline friend.\"  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load annotation with multiple choice question result data file\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import json\n",
    "\n",
    "\n",
    "# Data Directory: \n",
    "food_annotation_file_path = '/shared/data/upking/upking_annotation_with_image_captioning_result.json'\n",
    "\n",
    "\n",
    "# Method 1: Using pandas.read_json directly\n",
    "df = pd.read_json(food_annotation_file_path)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22eb2a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "# BLEU Evaluation (Average across multiple references)\n",
    "def evaluate_bleu(df):\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        references = row['captions']  # List of reference captions\n",
    "        candidate = row['predicted_caption']  # Predicted caption\n",
    "\n",
    "        # Tokenize the candidate and reference captions\n",
    "        tokenized_references = [ref.strip('\"').split() for ref in references]  # List of tokenized references\n",
    "        tokenized_candidate = candidate.strip('\"').split()  # Tokenized candidate\n",
    "\n",
    "        # Compute BLEU for all references\n",
    "        row_bleu_scores = [\n",
    "            sentence_bleu([ref], tokenized_candidate) for ref in tokenized_references\n",
    "        ]\n",
    "        \n",
    "        # Average across references\n",
    "        bleu_scores.append(sum(row_bleu_scores) / len(row_bleu_scores))\n",
    "    \n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    print(f\"Average BLEU: {avg_bleu:.4f}\")\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ecf440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0579048440249822e-231,\n",
       " 7.268137743732178e-232,\n",
       " 8.705390904782547e-156,\n",
       " 6.311011137147783e-232,\n",
       " 4.2637485317189243e-156,\n",
       " 8.691680632243206e-232,\n",
       " 7.140215571639016e-232,\n",
       " 7.751011273422461e-232,\n",
       " 9.930306306361165e-232,\n",
       " 3.7889263068295175e-155,\n",
       " 4.0784964889236096e-232,\n",
       " 9.126231777561599e-232,\n",
       " 2.2439443022320825e-155,\n",
       " 1.1119137413006488e-231,\n",
       " 3.324938679824386e-155,\n",
       " 1.614663279194461e-155,\n",
       " 1.1518583712221898e-155,\n",
       " 3.801257184539974e-232,\n",
       " 9.752516669670534e-156,\n",
       " 1.1214958877252064e-155]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bleu(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5b2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1191486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/upking/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# METEOR Evaluation (Average across multiple references)\n",
    "def evaluate_meteor(df):\n",
    "    meteor_scores = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        references = row['captions']  # List of reference captions\n",
    "        candidate = row['predicted_caption'].strip('\"')  # Predicted caption (raw string)\n",
    "\n",
    "        # Compute METEOR for all references\n",
    "        row_meteor_scores = [\n",
    "            meteor_score([[ref]], [candidate]) for ref in references\n",
    "        ]\n",
    "        \n",
    "        # Average across references\n",
    "        meteor_scores.append(sum(row_meteor_scores) / len(row_meteor_scores))\n",
    "    \n",
    "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "    print(f\"Average METEOR: {avg_meteor:.4f}\")\n",
    "    return meteor_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef6f6412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average METEOR: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_meteor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9438e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9946f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Evaluation (Average across multiple references)\n",
    "def evaluate_rouge(df):\n",
    "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        references = row['captions']  # List of reference captions\n",
    "        candidate = row['predicted_caption']  # Predicted caption\n",
    "        \n",
    "        # Compute ROUGE scores for all references\n",
    "        row_rouge1_scores, row_rouge2_scores, row_rougeL_scores = [], [], []\n",
    "        for ref in references:\n",
    "            scores = scorer.score(ref, candidate.strip('\"'))\n",
    "            row_rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "            row_rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "            row_rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "        \n",
    "        # Average across references\n",
    "        rouge1_scores.append(sum(row_rouge1_scores) / len(row_rouge1_scores))\n",
    "        rouge2_scores.append(sum(row_rouge2_scores) / len(row_rouge2_scores))\n",
    "        rougeL_scores.append(sum(row_rougeL_scores) / len(row_rougeL_scores))\n",
    "    \n",
    "    avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "    \n",
    "    print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n",
    "    print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n",
    "    print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n",
    "    return rouge1_scores, rouge2_scores, rougeL_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e707a720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1: 0.2365\n",
      "Average ROUGE-2: 0.0283\n",
      "Average ROUGE-L: 0.2213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.16600414078674947,\n",
       "  0.20055039559683524,\n",
       "  0.16126984126984129,\n",
       "  0.11281045751633986,\n",
       "  0.24015345268542196,\n",
       "  0.14026607894209578,\n",
       "  0.3167919799498746,\n",
       "  0.10538011695906432,\n",
       "  0.25705882352941173,\n",
       "  0.41301587301587295,\n",
       "  0.09180426556587548,\n",
       "  0.13356521739130436,\n",
       "  0.3705597326649958,\n",
       "  0.3145454545454545,\n",
       "  0.3806878306878307,\n",
       "  0.2590056022408963,\n",
       "  0.2875057208237986,\n",
       "  0.1550326797385621,\n",
       "  0.2686549707602339,\n",
       "  0.35558441558441556],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.04444444444444444,\n",
       "  0.0,\n",
       "  0.038095238095238085,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.09049707602339181,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0891640866873065,\n",
       "  0.07858585858585859,\n",
       "  0.05174825174825175,\n",
       "  0.049624060150375945,\n",
       "  0.06031746031746031,\n",
       "  0.0,\n",
       "  0.044444444444444446,\n",
       "  0.019999999999999997],\n",
       " [0.16600414078674947,\n",
       "  0.20055039559683524,\n",
       "  0.16126984126984129,\n",
       "  0.11281045751633986,\n",
       "  0.19923273657289,\n",
       "  0.12426607894209578,\n",
       "  0.2945697577276524,\n",
       "  0.10538011695906432,\n",
       "  0.25705882352941173,\n",
       "  0.3907936507936508,\n",
       "  0.09180426556587548,\n",
       "  0.13356521739130436,\n",
       "  0.3515121136173768,\n",
       "  0.2578787878787879,\n",
       "  0.33492063492063495,\n",
       "  0.2590056022408963,\n",
       "  0.24906178489702518,\n",
       "  0.1550326797385621,\n",
       "  0.2486549707602339,\n",
       "  0.3333621933621934])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rouge(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069d07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIDEr and SPICE (unchanged, since they handle multiple references internally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9a9ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cider(df):\n",
    "    ref_dict = {str(idx): row['captions'] for idx, row in df.iterrows()}\n",
    "    cand_dict = {str(idx): [row['predicted_caption']] for idx, row in df.iterrows()}\n",
    "    \n",
    "    cider_scorer = Cider()\n",
    "    score, _ = cider_scorer.compute_score(ref_dict, cand_dict)\n",
    "    print(f\"Average CIDEr: {score:.4f}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab777ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CIDEr: 0.0591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05907432453946156"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_cider(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618198b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3466d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_spice(df):\n",
    "    ref_dict = {str(idx): row['captions'] for idx, row in df.iterrows()}\n",
    "    cand_dict = {str(idx): [row['predicted_caption']] for idx, row in df.iterrows()}\n",
    "    \n",
    "    spice_scorer = Spice()\n",
    "    score, _ = spice_scorer.compute_score(ref_dict, cand_dict)\n",
    "    print(f\"Average SPICE: {score:.4f}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1614075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
