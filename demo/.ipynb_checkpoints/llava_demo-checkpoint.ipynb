{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351c00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8135a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6361a41a38854914b38b666f670c1904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model from local directory \n",
    "model_path = '/shared/model/llava-v1.6-mistral-7b-hf'\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_path)\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, load_in_4bit=True) \n",
    "#model.to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380e5fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]  \n",
      "From this image, which model shows strongest overral capacity? [/INST] The image you've provided appears to be a radar chart comparing the performance of various models across different tasks or datasets. The chart shows the scores of each model on each task, with the axes representing the different tasks or datasets and the radial lines representing the scores of each model on each task.\n",
      "\n",
      "From the chart, it's clear that the model with the highest scores across all tasks is the one labeled \"MMBENCH-CN.\" This model consistently outperforms the other models on each task, as indicated by the highest values on the radial lines.\n",
      "\n",
      "The other models, such as \"MMM-Vet,\" \"GQA,\" \"VizWiz,\" \"SQA-IMG,\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          # {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "          {\"type\": \"text\", \"text\": \"From this image, which model shows strongest overral capacity?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=150)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3d128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
