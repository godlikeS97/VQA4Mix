{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87af851b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_flash_attention_forward' from 'flash_attn.flash_attn_interface' (/opt/conda/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _flash_attention_forward\n\u001b[1;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_flash_attention_forward' from 'flash_attn.flash_attn_interface' (/opt/conda/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py)"
     ]
    }
   ],
   "source": [
    "# pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates\n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "from flash_attn.flash_attn_interface import _flash_attention_forward\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e35d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = \"/shared/model/BAAI/Aquila-VL-2B-llava-qwen\"\n",
    "\n",
    "model_name = \"llava_qwen\"\n",
    "device = \"cuda\"\n",
    "device_map = \"auto\"\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map)  # Add any other thing you want to pass in llava_model_args\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39988954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from url\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# load image from local environment\n",
    "# url = \"./local_image.jpg\"\n",
    "# image = Image.open(url)\n",
    "\n",
    "image_tensor = process_images([image], image_processor, model.config)\n",
    "image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "\n",
    "conv_template = \"qwen_1_5\"  # Make sure you use correct chat template for different models\n",
    "question = DEFAULT_IMAGE_TOKEN + \"\\nWhat is shown in this image?\"\n",
    "conv = copy.deepcopy(conv_templates[conv_template])\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt_question = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "image_sizes = [image.size]\n",
    "\n",
    "cont = model.generate(\n",
    "    input_ids,\n",
    "    images=image_tensor,\n",
    "    image_sizes=image_sizes,\n",
    "    do_sample=False,\n",
    "    temperature=0,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "\n",
    "text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\n",
    "\n",
    "print(text_outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
